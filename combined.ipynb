{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "93UYypewQOrI"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import cv2\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "\n",
        "tqdm.pandas()\n",
        "\n",
        "# Lazy imports so pylint compains, therefore we disable it\n",
        "from tensorflow.keras.utils import to_categorical  # type: ignore\n",
        "from tensorflow.keras import models, layers, optimizers  # type: ignore\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split, KFold\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import re\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [],
      "source": [
        "# If Google Colab\n",
        "is_colab_used = False\n",
        "folder_path = \"./data/full-dataset/train/clean\"\n",
        "if is_colab_used:\n",
        "    from google.colab import drive  # type: ignore\n",
        "\n",
        "    drive.mount(\"/content/drive\")\n",
        "    folder_path = \"/content/drive/My Drive/CS4243/\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 1. Collect All Raw Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of unique images: 7744\n"
          ]
        }
      ],
      "source": [
        "file_list = os.listdir(folder_path)\n",
        "df = pd.DataFrame(file_list, columns=[\"file_name\"])\n",
        "df[\"original_file_path\"] = df[\"file_name\"].apply(lambda x: os.path.join(folder_path, x))\n",
        "df[\"label\"] = df[\"file_name\"].str.split(\"-0\").str[0]\n",
        "\n",
        "print(f\"Number of unique images: {len(df['label'].unique())}\")\n",
        "\n",
        "# # Debug - Get subset of dataframe\n",
        "# df = df[0:100]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 2. Pre-process Data for Training the Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 7744/7744 [00:46<00:00, 164.89it/s]\n"
          ]
        }
      ],
      "source": [
        "def replace_black_with_surrounding_color_optimized(img):\n",
        "    result = img.copy()\n",
        "\n",
        "    height, width, _ = img.shape\n",
        "\n",
        "    # Create a mask where black pixels are True (shape: (height, width))\n",
        "    black_mask = np.all(img == [0, 0, 0], axis=-1)\n",
        "\n",
        "    # Initialize an array to accumulate the surrounding pixel values\n",
        "    accumulator = np.zeros_like(img, dtype=np.float32)\n",
        "\n",
        "    # Initialize a count of surrounding non-black pixels for each black pixel\n",
        "    surrounding_count = np.zeros((height, width), dtype=np.float32)\n",
        "\n",
        "    # Loop through the 8 surrounding pixels (dy, dx in {-1, 0, 1}, skipping (0, 0))\n",
        "    for dy in [-1, 0, 1]:\n",
        "        for dx in [-1, 0, 1]:\n",
        "            if dy == 0 and dx == 0:\n",
        "                continue  # Skip the black pixel itself\n",
        "\n",
        "            # Roll the image to get the surrounding pixels\n",
        "            rolled_img = np.roll(np.roll(img, shift=dy, axis=0), shift=dx, axis=1)\n",
        "\n",
        "            # Create a mask of the surrounding non-black pixels\n",
        "            non_black_mask = np.all(rolled_img != [0, 0, 0], axis=-1)\n",
        "\n",
        "            # Only accumulate colors from non-black pixels\n",
        "            accumulator += rolled_img * non_black_mask[..., np.newaxis]\n",
        "\n",
        "            # Update the surrounding count for non-black pixels\n",
        "            surrounding_count += non_black_mask\n",
        "\n",
        "    # Avoid division by zero: replace zeros in surrounding_count with 1 to prevent errors\n",
        "    surrounding_count[surrounding_count == 0] = 1\n",
        "\n",
        "    # For black pixels, replace them with the average color of surrounding pixels\n",
        "    result[black_mask] = (\n",
        "        accumulator[black_mask] / surrounding_count[black_mask, np.newaxis]\n",
        "    ).astype(np.uint8)\n",
        "\n",
        "    return result\n",
        "\n",
        "\n",
        "def process_image(image_path, output_folder):\n",
        "    # Load the image using OpenCV\n",
        "    img = cv2.imread(image_path)\n",
        "\n",
        "    if img is not None:\n",
        "        # Apply the optimized function to clean the image\n",
        "        cleaned_image = replace_black_with_surrounding_color_optimized(img)\n",
        "\n",
        "        if not os.path.exists(output_folder):\n",
        "            os.makedirs(output_folder)\n",
        "\n",
        "        # Construct the new file path (same filename, different folder)\n",
        "        output_path = os.path.join(output_folder, os.path.basename(image_path))\n",
        "\n",
        "        # Save the cleaned image to the new folder\n",
        "        cv2.imwrite(output_path, cleaned_image)\n",
        "\n",
        "        return output_path\n",
        "    else:\n",
        "        print(f\"Error: Could not load image at {image_path}\")\n",
        "        return None\n",
        "\n",
        "\n",
        "def process_images(image_paths, output_folder):\n",
        "    results = []\n",
        "    for path in tqdm(image_paths):\n",
        "        results.append(process_image(path, output_folder))\n",
        "    return results\n",
        "\n",
        "\n",
        "output_folder = os.path.join(\"./data\", \"cleaned_images\")\n",
        "df[\"processed_images\"] = process_images(\n",
        "    df[\"original_file_path\"].tolist(), output_folder\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 3. Format Data for Training the Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 7744/7744 [00:02<00:00, 2838.53it/s]\n"
          ]
        }
      ],
      "source": [
        "# Function to process images and extract ROIs\n",
        "output_char_folder = os.path.join(\"./data\", \"extracted_chars\")\n",
        "if not os.path.exists(output_char_folder):\n",
        "    os.makedirs(output_char_folder)\n",
        "contour_too_small_threshold = 8\n",
        "\n",
        "\n",
        "def extract_rois(image_path):\n",
        "    # Load the processed image\n",
        "    img = cv2.imread(image_path)\n",
        "\n",
        "    if img is not None:\n",
        "        # Convert to grayscale and threshold\n",
        "        gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "        _, thresh = cv2.threshold(gray, 254, 255, cv2.THRESH_BINARY_INV)\n",
        "\n",
        "        # Find contours\n",
        "        contours, _ = cv2.findContours(\n",
        "            thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE\n",
        "        )\n",
        "\n",
        "        img_with_contours = img.copy()\n",
        "        cv2.drawContours(img_with_contours, contours, -1, (0, 255, 75), 2)\n",
        "\n",
        "        # Sort contours from left to right\n",
        "        sorted_contours = sorted(contours, key=lambda ctr: cv2.boundingRect(ctr)[0])\n",
        "\n",
        "        rois = []  # List to store ROIs for display later\n",
        "        for contour in sorted_contours:\n",
        "            x, y, w, h = cv2.boundingRect(contour)\n",
        "            if w <= contour_too_small_threshold and h <= contour_too_small_threshold:\n",
        "                continue\n",
        "\n",
        "            # Draw bounding box on the image (optional)\n",
        "            cv2.rectangle(img_with_contours, (x, y), (x + w, y + h), (36, 255, 12), 2)\n",
        "\n",
        "            # Extract ROI and store it TESTINGGGGGG\n",
        "            ROI = img[y : y + h, x : x + w]\n",
        "\n",
        "            # Padding and resizing\n",
        "            if w > h:\n",
        "                # Add padding to the top and bottom\n",
        "                pad_top = (w - h) // 2\n",
        "                pad_bottom = w - h - pad_top\n",
        "                ROI_padded = cv2.copyMakeBorder(\n",
        "                    ROI,\n",
        "                    pad_top,\n",
        "                    pad_bottom,\n",
        "                    0,\n",
        "                    0,\n",
        "                    cv2.BORDER_CONSTANT,\n",
        "                    value=(255, 255, 255),\n",
        "                )\n",
        "            else:\n",
        "                # Add padding to the left and right\n",
        "                pad_left = (h - w) // 2\n",
        "                pad_right = h - w - pad_left\n",
        "                ROI_padded = cv2.copyMakeBorder(\n",
        "                    ROI,\n",
        "                    0,\n",
        "                    0,\n",
        "                    pad_left,\n",
        "                    pad_right,\n",
        "                    cv2.BORDER_CONSTANT,\n",
        "                    value=(255, 255, 255),\n",
        "                )\n",
        "\n",
        "            resized_ROI = cv2.resize(ROI_padded, (32, 32))\n",
        "            rois.append(resized_ROI)\n",
        "\n",
        "        return img_with_contours, rois  # Return both the image with contours and ROIs\n",
        "    else:\n",
        "        print(f\"Error: Could not load image at {image_path}\")\n",
        "        return None, []\n",
        "\n",
        "\n",
        "# Apply the ROI extraction to processed images in the DataFrame\n",
        "df[\"rois\"] = df[\"processed_images\"].progress_apply(lambda path: extract_rois(path))\n",
        "\n",
        "# # Debug - Display results\n",
        "# for index, (contours_image, rois) in enumerate(zip(df['rois'].apply(lambda x: x[0]), df['rois'].apply(lambda x: x[1]))):\n",
        "#     plt.figure(figsize=(10, 5))\n",
        "\n",
        "#     # Display image with bounding boxes\n",
        "#     plt.subplot(1, 2, 1)\n",
        "#     plt.imshow(cv2.cvtColor(contours_image, cv2.COLOR_BGR2RGB))\n",
        "#     plt.title(\"Image with Contours\")\n",
        "#     plt.axis('off')\n",
        "\n",
        "#     # Display ROIs\n",
        "#     plt.subplot(1, 2, 2)\n",
        "#     for i, roi in enumerate(rois):\n",
        "#         plt.subplot(1, len(rois), i + 1)\n",
        "#         plt.imshow(cv2.cvtColor(roi, cv2.COLOR_BGR2RGB))\n",
        "#         plt.title(f\"CHAR {i + 1}\")\n",
        "#         plt.axis('off')\n",
        "#     plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O2eh9w4aletw"
      },
      "source": [
        "# 4. Cleaned Data Collection For Training\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 484
        },
        "id": "HEv56k1llgnn",
        "outputId": "93a4eec0-30c0-475b-d382-eac9129153fd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "len(X):  7744\n",
            "len(y):  7744\n",
            "Sample image:\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "X = []\n",
        "Y = []\n",
        "\n",
        "folder_path = \"./data/cleaned_images\"\n",
        "file_list = os.listdir(folder_path)\n",
        "df = pd.DataFrame(file_list, columns=[\"file_name\"])\n",
        "df[\"original_file_path\"] = df[\"file_name\"].apply(lambda x: os.path.join(folder_path, x))\n",
        "df[\"label\"] = df[\"file_name\"].str.split(\"-0\").str[0]\n",
        "\n",
        "for filename in os.listdir(folder_path):\n",
        "    re_match = re.search(r\"(\\d+)(?=\\.png)\", filename)\n",
        "    if not re_match:\n",
        "        print(f\"Could not read {filename}, skipping\")\n",
        "        continue\n",
        "\n",
        "    digit = int(re_match.group(1))\n",
        "    if digit != 0 or not is_colab_used:\n",
        "        file_path = os.path.join(folder_path, filename)\n",
        "        image = cv2.imread(file_path, cv2.IMREAD_GRAYSCALE)\n",
        "\n",
        "        if image is not None:\n",
        "            # Resize the image to 32x32 pixels\n",
        "            resized_image = cv2.resize(image, (32, 32))\n",
        "            X.append(resized_image)\n",
        "            Y.append(filename[digit - 1])\n",
        "        else:\n",
        "            print(f\"Could not read {filename} from {file_path}, skipping\")\n",
        "\n",
        "print(\"len(X): \", len(X))\n",
        "print(\"len(y): \", len(Y))\n",
        "print(\"Sample image:\")\n",
        "rand_idx = np.random.randint(0, len(X))\n",
        "plt.imshow(cv2.cvtColor(X[rand_idx], cv2.COLOR_BGR2RGB))\n",
        "\n",
        "X = np.array(X)\n",
        "Y = np.array(Y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pzjdVkGXm4NT"
      },
      "source": [
        "# 5. Preparing Processed Data for Training\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "47d97doTm3_O"
      },
      "outputs": [],
      "source": [
        "digits = [str(i) for i in range(10)]\n",
        "letters = [chr(i) for i in range(ord(\"a\"), ord(\"z\") + 1)]\n",
        "char_list = digits + letters\n",
        "\n",
        "\n",
        "# Normalize\n",
        "def normalize_data(data):\n",
        "    return data / 255.0\n",
        "\n",
        "\n",
        "# One hot encoding\n",
        "def one_hot_encode_labels(labels):\n",
        "    label_encoder = LabelEncoder()\n",
        "    label_encoder.fit(char_list)\n",
        "\n",
        "    encoded = label_encoder.transform(labels)\n",
        "    categorical = to_categorical(encoded, num_classes=len(char_list))\n",
        "\n",
        "    return categorical\n",
        "\n",
        "\n",
        "X = normalize_data(X)\n",
        "Y = one_hot_encode_labels(Y)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, Y, test_size=0.2, random_state=42\n",
        ")\n",
        "X_train = X_train.reshape((X_train.shape[0], X_train.shape[1], X_train.shape[2], 1))\n",
        "X_test = X_test.reshape((X_test.shape[0], X_test.shape[1], X_test.shape[2], 1))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ejweuRoZlg2v"
      },
      "source": [
        "# 6. Building Model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "Qt9QhXVMWhPr"
      },
      "outputs": [],
      "source": [
        "# Build Model Function\n",
        "def build_model(learning_rate=0.01, dropout_rate=0.5):\n",
        "    model = models.Sequential()\n",
        "    model.add(\n",
        "        layers.Conv2D(\n",
        "            filters=32, kernel_size=(3, 3), activation=\"relu\", input_shape=(32, 32, 1)\n",
        "        )\n",
        "    )\n",
        "    model.add(layers.MaxPooling2D(pool_size=(2, 2)))\n",
        "    model.add(layers.BatchNormalization())\n",
        "    model.add(layers.Flatten())\n",
        "    model.add(layers.Dense(64, activation=\"relu\"))\n",
        "    model.add(layers.Dropout(dropout_rate))\n",
        "    model.add(layers.Dense(36, activation=\"softmax\"))  # 0-9, a-z\n",
        "\n",
        "    model.compile(\n",
        "        optimizer=optimizers.Adam(learning_rate=learning_rate),\n",
        "        loss=\"categorical_crossentropy\",\n",
        "        metrics=[\"accuracy\"],\n",
        "    )\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "# Train Model Function\n",
        "def train_model(X, y):\n",
        "    label_encoder = LabelEncoder()\n",
        "    label_encoder.fit(char_list)\n",
        "\n",
        "    y_encoded = label_encoder.transform(y)\n",
        "    y_categorical = to_categorical(y_encoded, num_classes=36)\n",
        "\n",
        "    model = build_model(input_shape=X.shape[1:])  # Using input shape from X\n",
        "    model.compile(\n",
        "        optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"]\n",
        "    )\n",
        "    model.fit(X, y_categorical, epochs=10, batch_size=32)\n",
        "\n",
        "    return model, label_encoder\n",
        "\n",
        "\n",
        "# Predict Function\n",
        "def predict_model(model, label_encoder, X):\n",
        "    predictions = model.predict(X)\n",
        "    predicted_classes = np.argmax(predictions, axis=1)\n",
        "    predicted_labels = label_encoder.inverse_transform(predicted_classes)\n",
        "    return predicted_labels"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F-eG8DIilWyd"
      },
      "source": [
        "# 7. Cross Validation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "98oyzky9ZBzM",
        "outputId": "24356784-9c08-482d-b3eb-79bd082ae01f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/vishnusundaresan/Documents/School/CS4243/.venv/lib/python3.11/site-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.9357 - loss: 0.3134 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
            "Epoch 2/10\n",
            "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.9991 - loss: 0.0238 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
            "Epoch 3/10\n",
            "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.9996 - loss: 0.0266 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
            "Epoch 4/10\n",
            "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.9991 - loss: 0.0502 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
            "Epoch 5/10\n",
            "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.9996 - loss: 0.0139 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
            "Epoch 6/10\n",
            "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.9999 - loss: 0.0012 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
            "Epoch 7/10\n",
            "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.9990 - loss: 0.0111 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
            "Epoch 8/10\n",
            "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.9993 - loss: 0.0022 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
            "Epoch 9/10\n",
            "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.9996 - loss: 0.0068 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
            "Epoch 10/10\n",
            "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.9986 - loss: 0.0078 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
            "Epoch 1/10\n",
            "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.9347 - loss: 0.3017 - val_accuracy: 0.9992 - val_loss: 0.2013\n",
            "Epoch 2/10\n",
            "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.9999 - loss: 0.0140 - val_accuracy: 0.9992 - val_loss: 0.1210\n",
            "Epoch 3/10\n",
            "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.9964 - loss: 0.2262 - val_accuracy: 0.9992 - val_loss: 0.0714\n",
            "Epoch 4/10\n",
            "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.9999 - loss: 0.0056 - val_accuracy: 0.9992 - val_loss: 0.1283\n",
            "Epoch 5/10\n",
            "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.9999 - loss: 0.0064 - val_accuracy: 0.9992 - val_loss: 0.1267\n",
            "Epoch 6/10\n",
            "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.9997 - loss: 0.0011 - val_accuracy: 0.9992 - val_loss: 0.2682\n",
            "Epoch 7/10\n",
            "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.9997 - loss: 0.0512 - val_accuracy: 0.9992 - val_loss: 0.0763\n",
            "Epoch 8/10\n",
            "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.9998 - loss: 0.0012 - val_accuracy: 0.9992 - val_loss: 0.1209\n",
            "Epoch 9/10\n",
            "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.9992 - loss: 0.0116 - val_accuracy: 0.9992 - val_loss: 0.2635\n",
            "Epoch 10/10\n",
            "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.9996 - loss: 0.0017 - val_accuracy: 0.9992 - val_loss: 0.2369\n",
            "Epoch 1/10\n",
            "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.9359 - loss: 0.3247 - val_accuracy: 0.9992 - val_loss: 0.1145\n",
            "Epoch 2/10\n",
            "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.9991 - loss: 0.0957 - val_accuracy: 0.9992 - val_loss: 0.0338\n",
            "Epoch 3/10\n",
            "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9999 - loss: 0.0047 - val_accuracy: 0.9992 - val_loss: 0.0282\n",
            "Epoch 4/10\n",
            "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.9997 - loss: 0.0127 - val_accuracy: 0.9992 - val_loss: 0.0888\n",
            "Epoch 5/10\n",
            "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.9997 - loss: 0.0125 - val_accuracy: 0.9992 - val_loss: 0.0898\n",
            "Epoch 6/10\n",
            "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.9996 - loss: 0.0012 - val_accuracy: 0.9992 - val_loss: 0.1057\n",
            "Epoch 7/10\n",
            "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 0.0011 - val_accuracy: 0.9992 - val_loss: 0.0864\n",
            "Epoch 8/10\n",
            "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.9996 - loss: 0.0058 - val_accuracy: 0.9992 - val_loss: 0.1863\n",
            "Epoch 9/10\n",
            "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.9998 - loss: 0.0018 - val_accuracy: 0.9992 - val_loss: 0.0425\n",
            "Epoch 10/10\n",
            "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.9996 - loss: 0.0042 - val_accuracy: 0.9992 - val_loss: 0.1916\n",
            "Epoch 1/10\n",
            "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.9378 - loss: 0.2734 - val_accuracy: 0.0000e+00 - val_loss: 8.6164\n",
            "Epoch 2/10\n",
            "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9971 - loss: 0.0758 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
            "Epoch 3/10\n",
            "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.9999 - loss: 0.0072 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
            "Epoch 4/10\n",
            "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.9994 - loss: 0.0091 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
            "Epoch 5/10\n",
            "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.9987 - loss: 0.0821 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
            "Epoch 6/10\n",
            "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9996 - loss: 0.0070 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
            "Epoch 7/10\n",
            "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9987 - loss: 0.0096 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
            "Epoch 8/10\n",
            "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9994 - loss: 0.0273 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
            "Epoch 9/10\n",
            "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9970 - loss: 0.0482 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
            "Epoch 10/10\n",
            "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.9992 - loss: 0.0080 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
            "Epoch 1/10\n",
            "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.9396 - loss: 0.3179 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
            "Epoch 2/10\n",
            "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.9996 - loss: 0.0205 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
            "Epoch 3/10\n",
            "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9995 - loss: 0.0567 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
            "Epoch 4/10\n",
            "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9994 - loss: 0.0122 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
            "Epoch 5/10\n",
            "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.9995 - loss: 0.0257 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
            "Epoch 6/10\n",
            "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.9991 - loss: 0.0178 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
            "Epoch 7/10\n",
            "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.9994 - loss: 0.0035 - val_accuracy: 1.0000 - val_loss: 9.6214e-11\n",
            "Epoch 8/10\n",
            "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.9994 - loss: 0.0050 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
            "Epoch 9/10\n",
            "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.9992 - loss: 0.0184 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
            "Epoch 10/10\n",
            "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9994 - loss: 0.0276 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
            "Cross-Validation Accuracies: [1.0, 0.9991928935050964, 0.9991928935050964, 1.0, 1.0]\n",
            "Mean Accuracy: 0.9996771574020386\n"
          ]
        }
      ],
      "source": [
        "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "kf_accuracy = []\n",
        "\n",
        "for train_idx, val_idx in kf.split(X_train):\n",
        "    X_train_fold, X_val_fold = X_train[train_idx], X_train[val_idx]\n",
        "    y_train_fold, y_val_fold = y_train[train_idx], y_train[val_idx]\n",
        "\n",
        "    model = build_model()\n",
        "\n",
        "    model.fit(\n",
        "        X_train_fold,\n",
        "        y_train_fold,\n",
        "        epochs=10,\n",
        "        validation_data=(X_val_fold, y_val_fold),\n",
        "        batch_size=64,\n",
        "    )\n",
        "\n",
        "    val_loss, val_acc = model.evaluate(X_val_fold, y_val_fold, verbose=0)\n",
        "    kf_accuracy.append(val_acc)\n",
        "\n",
        "print(f\"Cross-Validation Accuracies: {kf_accuracy}\")\n",
        "print(f\"Mean Accuracy: {np.mean(kf_accuracy)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6BznBWyjqQpQ"
      },
      "source": [
        "# 8. Hyperparameter Tuning\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7ubRWab_mGJk",
        "outputId": "eebdf6fa-6fe9-4903-c73f-9d399dbc78c2"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/vishnusundaresan/Documents/School/CS4243/.venv/lib/python3.11/site-packages/scikeras/wrappers.py:925: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
            "  X, y = self._initialize(X, y)\n",
            "/Users/vishnusundaresan/Documents/School/CS4243/.venv/lib/python3.11/site-packages/scikeras/wrappers.py:925: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
            "  X, y = self._initialize(X, y)\n",
            "/Users/vishnusundaresan/Documents/School/CS4243/.venv/lib/python3.11/site-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
            "/Users/vishnusundaresan/Documents/School/CS4243/.venv/lib/python3.11/site-packages/scikeras/wrappers.py:925: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
            "  X, y = self._initialize(X, y)\n",
            "/Users/vishnusundaresan/Documents/School/CS4243/.venv/lib/python3.11/site-packages/scikeras/wrappers.py:925: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
            "  X, y = self._initialize(X, y)\n",
            "/Users/vishnusundaresan/Documents/School/CS4243/.venv/lib/python3.11/site-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
            "/Users/vishnusundaresan/Documents/School/CS4243/.venv/lib/python3.11/site-packages/scikeras/wrappers.py:925: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
            "  X, y = self._initialize(X, y)\n",
            "/Users/vishnusundaresan/Documents/School/CS4243/.venv/lib/python3.11/site-packages/scikeras/wrappers.py:925: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
            "  X, y = self._initialize(X, y)\n",
            "/Users/vishnusundaresan/Documents/School/CS4243/.venv/lib/python3.11/site-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
            "/Users/vishnusundaresan/Documents/School/CS4243/.venv/lib/python3.11/site-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
            "/Users/vishnusundaresan/Documents/School/CS4243/.venv/lib/python3.11/site-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
            "/Users/vishnusundaresan/Documents/School/CS4243/.venv/lib/python3.11/site-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
            "/Users/vishnusundaresan/Documents/School/CS4243/.venv/lib/python3.11/site-packages/scikeras/wrappers.py:925: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
            "  X, y = self._initialize(X, y)\n",
            "/Users/vishnusundaresan/Documents/School/CS4243/.venv/lib/python3.11/site-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
            "/Users/vishnusundaresan/Documents/School/CS4243/.venv/lib/python3.11/site-packages/scikeras/wrappers.py:925: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
            "  X, y = self._initialize(X, y)\n",
            "/Users/vishnusundaresan/Documents/School/CS4243/.venv/lib/python3.11/site-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
            "/Users/vishnusundaresan/Documents/School/CS4243/.venv/lib/python3.11/site-packages/scikeras/wrappers.py:925: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
            "  X, y = self._initialize(X, y)\n",
            "/Users/vishnusundaresan/Documents/School/CS4243/.venv/lib/python3.11/site-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
            "/Users/vishnusundaresan/Documents/School/CS4243/.venv/lib/python3.11/site-packages/scikeras/wrappers.py:925: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
            "  X, y = self._initialize(X, y)\n",
            "/Users/vishnusundaresan/Documents/School/CS4243/.venv/lib/python3.11/site-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
            "/Users/vishnusundaresan/Documents/School/CS4243/.venv/lib/python3.11/site-packages/scikeras/wrappers.py:925: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
            "  X, y = self._initialize(X, y)\n",
            "/Users/vishnusundaresan/Documents/School/CS4243/.venv/lib/python3.11/site-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
            "/Users/vishnusundaresan/Documents/School/CS4243/.venv/lib/python3.11/site-packages/scikeras/wrappers.py:925: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
            "  X, y = self._initialize(X, y)\n",
            "/Users/vishnusundaresan/Documents/School/CS4243/.venv/lib/python3.11/site-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
            "/Users/vishnusundaresan/Documents/School/CS4243/.venv/lib/python3.11/site-packages/scikeras/wrappers.py:925: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
            "  X, y = self._initialize(X, y)\n",
            "/Users/vishnusundaresan/Documents/School/CS4243/.venv/lib/python3.11/site-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
            "/Users/vishnusundaresan/Documents/School/CS4243/.venv/lib/python3.11/site-packages/scikeras/wrappers.py:925: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
            "  X, y = self._initialize(X, y)\n",
            "/Users/vishnusundaresan/Documents/School/CS4243/.venv/lib/python3.11/site-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
            "/Users/vishnusundaresan/Documents/School/CS4243/.venv/lib/python3.11/site-packages/scikeras/wrappers.py:925: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
            "  X, y = self._initialize(X, y)\n",
            "/Users/vishnusundaresan/Documents/School/CS4243/.venv/lib/python3.11/site-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
            "/Users/vishnusundaresan/Documents/School/CS4243/.venv/lib/python3.11/site-packages/scikeras/wrappers.py:925: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
            "  X, y = self._initialize(X, y)\n",
            "/Users/vishnusundaresan/Documents/School/CS4243/.venv/lib/python3.11/site-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
            "/Users/vishnusundaresan/Documents/School/CS4243/.venv/lib/python3.11/site-packages/scikeras/wrappers.py:925: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
            "  X, y = self._initialize(X, y)\n",
            "/Users/vishnusundaresan/Documents/School/CS4243/.venv/lib/python3.11/site-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
            "/Users/vishnusundaresan/Documents/School/CS4243/.venv/lib/python3.11/site-packages/scikeras/wrappers.py:925: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
            "  X, y = self._initialize(X, y)\n",
            "/Users/vishnusundaresan/Documents/School/CS4243/.venv/lib/python3.11/site-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
            "/Users/vishnusundaresan/Documents/School/CS4243/.venv/lib/python3.11/site-packages/scikeras/wrappers.py:925: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
            "  X, y = self._initialize(X, y)\n",
            "/Users/vishnusundaresan/Documents/School/CS4243/.venv/lib/python3.11/site-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
            "/Users/vishnusundaresan/Documents/School/CS4243/.venv/lib/python3.11/site-packages/scikeras/wrappers.py:925: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
            "  X, y = self._initialize(X, y)\n",
            "/Users/vishnusundaresan/Documents/School/CS4243/.venv/lib/python3.11/site-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
            "/Users/vishnusundaresan/Documents/School/CS4243/.venv/lib/python3.11/site-packages/scikeras/wrappers.py:925: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
            "  X, y = self._initialize(X, y)\n",
            "/Users/vishnusundaresan/Documents/School/CS4243/.venv/lib/python3.11/site-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
            "/Users/vishnusundaresan/Documents/School/CS4243/.venv/lib/python3.11/site-packages/scikeras/wrappers.py:925: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
            "  X, y = self._initialize(X, y)\n",
            "/Users/vishnusundaresan/Documents/School/CS4243/.venv/lib/python3.11/site-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
            "/Users/vishnusundaresan/Documents/School/CS4243/.venv/lib/python3.11/site-packages/scikeras/wrappers.py:925: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
            "  X, y = self._initialize(X, y)\n",
            "/Users/vishnusundaresan/Documents/School/CS4243/.venv/lib/python3.11/site-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
            "/Users/vishnusundaresan/Documents/School/CS4243/.venv/lib/python3.11/site-packages/scikeras/wrappers.py:925: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
            "  X, y = self._initialize(X, y)\n",
            "/Users/vishnusundaresan/Documents/School/CS4243/.venv/lib/python3.11/site-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
            "/Users/vishnusundaresan/Documents/School/CS4243/.venv/lib/python3.11/site-packages/scikeras/wrappers.py:925: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
            "  X, y = self._initialize(X, y)\n",
            "/Users/vishnusundaresan/Documents/School/CS4243/.venv/lib/python3.11/site-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
            "/Users/vishnusundaresan/Documents/School/CS4243/.venv/lib/python3.11/site-packages/scikeras/wrappers.py:925: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
            "  X, y = self._initialize(X, y)\n",
            "/Users/vishnusundaresan/Documents/School/CS4243/.venv/lib/python3.11/site-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
            "/Users/vishnusundaresan/Documents/School/CS4243/.venv/lib/python3.11/site-packages/scikeras/wrappers.py:925: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
            "  X, y = self._initialize(X, y)\n",
            "/Users/vishnusundaresan/Documents/School/CS4243/.venv/lib/python3.11/site-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
            "/Users/vishnusundaresan/Documents/School/CS4243/.venv/lib/python3.11/site-packages/scikeras/wrappers.py:925: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
            "  X, y = self._initialize(X, y)\n",
            "/Users/vishnusundaresan/Documents/School/CS4243/.venv/lib/python3.11/site-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best Hyperparameters: {'model__dropout_rate': 0.0, 'model__learning_rate': 0.01}\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "from scikeras.wrappers import KerasClassifier\n",
        "\n",
        "model = KerasClassifier(build_fn=build_model, verbose=0)\n",
        "\n",
        "# Define the grid of hyperparameters\n",
        "param_grid = {\n",
        "    \"model__learning_rate\": [0.001, 0.01, 0.1],\n",
        "    \"model__dropout_rate\": [0.0, 0.2, 0.5],\n",
        "}\n",
        "\n",
        "# Conduct the grid search\n",
        "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1, cv=3)\n",
        "grid_result = grid.fit(X_train, y_train)\n",
        "\n",
        "# Print the best hyperparameters\n",
        "print(f\"Best Hyperparameters: {grid_result.best_params_}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 9. Build and Save the Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xL20qMHPqX9s",
        "outputId": "b5f83ac2-5432-43e3-da12-6f661c108905"
      },
      "outputs": [],
      "source": [
        "model = build_model(learning_rate=0.001, dropout_rate=0.5)\n",
        "\n",
        "# Save the model\n",
        "from datetime import datetime\n",
        "from zoneinfo import ZoneInfo\n",
        "\n",
        "model.save(f\"model-{datetime.now(tz=ZoneInfo('UTC'))}.keras\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
